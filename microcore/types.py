from typing import TYPE_CHECKING, Callable, Any, Awaitable, Union, List
from os import PathLike

from .message_types import Msg

if TYPE_CHECKING:
    from .wrappers.prompt_wrapper import PromptWrapper  # noqa: F401
    from .wrappers.llm_response_wrapper import LLMResponse  # noqa: F401

TPrompt = Union[
    dict, Msg, str, "PromptWrapper", List[Union[dict, Msg, str, "PromptWrapper"]]
]
"""Type for prompt argument in LLM requests"""
TplFunctionType = Callable[[Union[PathLike[str], str], Any], str]
"""Function type for rendering prompt templates"""
LLMFunctionType = Callable[[TPrompt, Any], "LLMResponse"]
"""Function type for requesting LLM synchronously"""
LLMAsyncFunctionType = Callable[[TPrompt, Any], Awaitable["LLMResponse"]]
"""Function type for requesting LLM asynchronously"""


class BadAIAnswer(ValueError):
    """Unprocessable response generated by the LLM"""

    def __init__(self, message: str = None, details: str = None):
        self.message = str(message or "Unprocessable response generated by the LLM")
        self.details = details
        super().__init__(self.message + (f": {self.details}" if self.details else ""))

    def __str__(self):
        return self.message + (f": {self.details}" if self.details else "")


class BadAIJsonAnswer(BadAIAnswer):
    def __init__(
        self, message: str = "Invalid JSON generated by the LLM", details=None
    ):
        super().__init__(message, details)
